{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f3c53b",
   "metadata": {},
   "source": [
    "# Assignment 2 - Elements of Data Processing COMP20008\n",
    "\n",
    "\n",
    "# Contributors:\n",
    "### **Euan Marshall - 1480650**\n",
    "### **Di Wang - 1353637**\n",
    "### **Harry Huppatz - 1357194**\n",
    "### **Quoc Minh Hoang - 1386263**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb97db",
   "metadata": {},
   "source": [
    "## **Part 1: Data Exploration and Analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries, exploring the dataset and cleaning missing values.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "titles = pd.read_csv('titles.csv')\n",
    "display(titles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f3be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the built-in pandas function we can return a \n",
    "# DataFrame of the Pearson's correlation between each of the continuously measured features.\n",
    "features = ['tmdb_popularity', 'tmdb_score', 'release_year', 'imdb_votes', 'imdb_score']\n",
    "titles[features].corr(method = 'pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.dropna(subset=['imdb_score'], inplace=True)\n",
    "non_numeric_mask = pd.to_numeric(titles['imdb_score'], errors='coerce').isna()\n",
    "\n",
    "# Filter the DataFrame to get rows with non-numeric values\n",
    "non_numeric_values = titles[non_numeric_mask]\n",
    "\n",
    "# Display the result\n",
    "print(non_numeric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe718f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Pearson and Spearman rank coefficients.\n",
    "\n",
    "pearson_corr = titles.corr(method='pearson')['imdb_score']\n",
    "spearman_corr = titles.corr(method='spearman')['imdb_score']\n",
    "\n",
    "# Printing Pearson and Spearman rank coefficients.\n",
    "print(\"Pearson Correlation Coefficients:\")\n",
    "print(pearson_corr)\n",
    "print(\"\\nSpearman Rank Correlation Coefficients:\")\n",
    "print(spearman_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pair plot to visualise the correlations between the numerical features. \n",
    "# Analysing spearman and pearson correlations.\n",
    "import seaborn as sns\n",
    "\n",
    "titles.dropna(inplace=True) # Dropping NaN values for purpose of quick viewing.\n",
    "\n",
    "# Creating pariplot.\n",
    "sns.pairplot(titles, height=1.2, aspect=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking NaN values in each column.\n",
    "df = pd.read_csv('titles.csv')\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the correlation between IMDb score and TMDb score.\n",
    "df = pd.read_csv('titles.csv')\n",
    "\n",
    "# Creating a scatter plot.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['tmdb_score'], df['imdb_score'], alpha=0.2)\n",
    "plt.xlabel('TMDb Score')\n",
    "plt.ylabel('IMDb Score')\n",
    "plt.title('TMDB Score vs. IMDb Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde628fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the correlation between IMDb score and TMDb score.\n",
    "df = pd.read_csv('titles.csv')\n",
    "\n",
    "# Creating a scatter plot.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['imdb_votes'], df['imdb_score'], alpha=0.5)\n",
    "plt.xlabel('IMDB Votes')\n",
    "plt.ylabel('IMDb Score')\n",
    "plt.title('IMDB Votes vs. IMDb Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c49e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exploring the correlation between IMDb score and TMDb score.\n",
    "df = pd.read_csv('titles.csv')\n",
    "\n",
    "# Creating a scatter plot.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['tmdb_popularity'], df['imdb_score'], alpha=0.2)\n",
    "plt.xlabel('TMDb Popularity')\n",
    "plt.ylabel('IMDb Score')\n",
    "plt.title('TMDB Popularity vs. IMDb Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6a188",
   "metadata": {},
   "source": [
    "## **Part 2: Datafile Cleaning and Imputating.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506abadb",
   "metadata": {},
   "source": [
    "**The aim here is to fill the missing IMDb values, as these are what we are trying to predict.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing IMDb score, TMDB score and IMDb votes.\n",
    "missing_imdb = df['imdb_score'].isnull()\n",
    "missing_tmdb = df['tmdb_score'].isnull()\n",
    "missing_votes = df['imdb_votes'].isnull()\n",
    "\n",
    "# Creating a condition to identify data points with missing IMDb and at least one of the other columns.\n",
    "missing_condition = (missing_imdb &\n",
    "                     (missing_tmdb | missing_votes))\n",
    "\n",
    "# Counting the number of data points that meet the condition for each missing column.\n",
    "count_missing_tmdb = len(df[missing_condition & missing_tmdb])\n",
    "count_missing_votes = len(df[missing_condition & missing_votes])\n",
    "\n",
    "print(f'Number of data points missing IMDb score and also missing TMDB score: {count_missing_tmdb}')\n",
    "print(f'Number of data points missing IMDb score and also missing IMDb votes: {count_missing_votes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2577b",
   "metadata": {},
   "source": [
    "**Evidently, there is some correlation between IMDb and TMDb scores, as well as between IMDb scores and IMDb votes. However, for all of the data points missing IMDb scores, they are also missing IMDb votes values. Hence, we reject the idea of using IMDb votes as a feature for imputation of IMDb scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding data points with 10/10 TMDb scores, and the total number of them.\n",
    "df = pd.read_csv('titles.csv')\n",
    "\n",
    "# Filtering the DataFrame to get rows with TMDB scores equal to 10.\n",
    "tmdb_10 = df[df['tmdb_score'] == 10]\n",
    "\n",
    "# Displaying the rows with 10/10 TMDB scores.\n",
    "display(tmdb_10)\n",
    "print('Number of perfect TMDb score values: ', len(tmdb_10))\n",
    "print('Percentage of total dataframe ', len(tmdb_10)/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242d810",
   "metadata": {},
   "source": [
    "**We also recognise the presence of data points with 10/10 TMDb scores but much lower IMDb scores. After further investigation, these are because they are not listed on the TMDb website. So we delete these values (as they make just 1.2% of the dataset, and TMDb scores will be used to predict missing IMDb scores due to their strong(er) correlation). This is an area for improvement, as deletion of values is not ideal, but incorrect data is also harmful to our imputation methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing values where tmdb_score is 10 and where description is null.\n",
    "df = df[df['tmdb_score'] != 10]\n",
    "df.dropna(subset=['description'], inplace=True)\n",
    "df.to_csv('fixed_titles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952cd39",
   "metadata": {},
   "source": [
    "**USING MACHINE LEARNING MODEL TO FILL MISSING VALUES**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any data rows that are missing both imdb_score and tmdb_score.\n",
    "missing_scores = df[df['tmdb_score'].isna() & df['imdb_score'].isna()]\n",
    "\n",
    "print('Number of data points with missing IMDb and TMDb scores is', len(missing_scores))\n",
    "\n",
    "print('The percentage of the whole dataset is', len(missing_scores)/5850)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd32516",
   "metadata": {},
   "source": [
    "**From the calculation above, there are now 81 data values which are missing both TMDb and IMDb scores. Because we aim to impute the IMDb score from the TMDb score, these values must be eliminated. We therefore lose a further 1.3% of the data at this stage, which is reasonable considering there is no accurate means of filling these missing IMDb data points given the most strongly correlated value TMDb score is also missing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping data values with NaN values for both TMDb and IMDb scores.\n",
    "df.dropna(subset=['tmdb_score', 'imdb_score'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcff218",
   "metadata": {},
   "source": [
    "**We now build a model using decision trees to predict the missing IMDb scores given the TMDb scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361da4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, removing the missing IMDb records from the dataset. \n",
    "missing_imdb_score = df[df['imdb_score'].isnull() &\n",
    "                        df['tmdb_score'].notnull()]\n",
    "\n",
    "missing_imdb_score.to_csv('imdb_score.csv', index=False)\n",
    "\n",
    "df_without_missing_imdb = df\n",
    "df_without_missing_imdb.dropna(subset=['imdb_score'], inplace = True)\n",
    "\n",
    "df_without_missing_imdb.to_csv('fixed_training_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('fixed_training_titles.csv')\n",
    "\n",
    "def train_model(df, features, target):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Train the models\n",
    "model1, X1_test, y1_test = train_model(df, ['tmdb_score'], 'imdb_score')\n",
    "                                       \n",
    "score1 = model1.score(X1_test, y1_test)\n",
    "print(f'Decision Tree R^2 Score: {score1}')\n",
    "\n",
    "scores1 = cross_val_score(model1, X1_test, y1_test, cv=10)\n",
    "print(f'Decision Tree Cross-Validation Score: {scores1.mean()}')\n",
    "\n",
    "y1_pred = model1.predict(X1_test)\n",
    "mse1 = mean_squared_error(y1_test, y1_pred)\n",
    "mae1 = mean_absolute_error(y1_test, y1_pred)\n",
    "print(\"Mean Squared Error for Decision Tree:\", mse1)\n",
    "print(\"Mean Absolute Error for Decision Tree:\", mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff51de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution of actual values in blue\n",
    "sns.histplot(y1_test, bins=10, color='blue', alpha=0.5, label='Actual', kde=True)\n",
    "\n",
    "# Plot the distribution of predicted values in red\n",
    "sns.histplot(y1_pred, bins=10, color='red', alpha=0.5, label='Predicted', kde=True)\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('IMDb Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted Values for Decision Tree Imputation Model')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ef6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_model(df, features, target):\n",
    "    '''Trains Decision Tree models on the provided data.'''\n",
    "\n",
    "    # Defining the features and the target for the model.\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Splitting the data into training and testing sets.\n",
    "    # A common rule is to use about 80% of the data for training and about 20% of the data for testing.\n",
    "    # A common choice is to set random_state to 0 or 42.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initializing the model.\n",
    "    # Fitting the model to the training data.\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "\n",
    "# Defining the features.\n",
    "selected_features = ['tmdb_score', 'tmdb_popularity']\n",
    "\n",
    "# Training the models.\n",
    "model2, X2_test, y2_test = train_model(df, selected_features, 'imdb_score')\n",
    "\n",
    "score2 = model2.score(X2_test, y2_test)\n",
    "print(f'Decision Tree Model2 R^2 Score: {score1}')\n",
    "\n",
    "scores2 = cross_val_score(model2, X2_test, y2_test, cv=10)\n",
    "print(f'Decision Tree Model2 Cross-Validation Score: {scores2.mean()}')\n",
    "\n",
    "y2_pred = model2.predict(X2_test)\n",
    "mse2 = mean_squared_error(y2_test, y2_pred)\n",
    "mae2 = mean_absolute_error(y2_test, y2_pred)\n",
    "print(\"Mean Squared Error for model2:\", mse2)\n",
    "print(\"Mean Absolute Error for model2:\", mae2)\n",
    "\n",
    "# Creating a figure and axis.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the distribution of actual IMDb scores in blue.\n",
    "sns.histplot(y2_test, bins=10, color='blue', alpha=0.5, label='Actual', kde=True)\n",
    "\n",
    "# Plotting the distribution of predicted IMDb scores in red.\n",
    "sns.histplot(y2_pred, bins=10, color='red', alpha=0.5, label='Predicted', kde=True)\n",
    "\n",
    "# Adding labels and a legend.\n",
    "plt.xlabel('IMDb Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted Values for Decision Tree Model')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot.\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39881db9",
   "metadata": {},
   "source": [
    "**Evidently, this means of imputation is not very accurate. So we try other models.**\n",
    "**We will next try a K-NN model for imputation of missing IMDb scores, using the TMDb score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30058bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('fixed_training_titles.csv')\n",
    "\n",
    "# Dropping rows with missing values in specified columns.\n",
    "df.dropna(subset=['tmdb_score', 'imdb_score'], inplace=True)  \n",
    "\n",
    "def train_model(df, features, target):\n",
    "    '''Trains K-NN models on the provided data.'''\n",
    "\n",
    "    # Defining the features and the target for the model.\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Splitting the data into training and testing sets.\n",
    "    # A common rule is to use about 80% of the data for training and about 20% of the data for testing.\n",
    "    # A common choice is to set random_state to 0 or 42.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the model.\n",
    "    # Fitting the model to the training data.\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Training the models with 'tmdb_score' and 'runtime' as features.\n",
    "model1, X1_test, y1_test = train_model(df, ['tmdb_score'], 'imdb_score')\n",
    "\n",
    "score1 = model1.score(X1_test, y1_test)\n",
    "print(f'K-NN Model1 R^2 Score: {score1}')\n",
    "\n",
    "scores1 = cross_val_score(model1, X1_test, y1_test, cv=10)\n",
    "print(f'K-NN Model1 Cross-Validation Score: {scores1.mean()}')\n",
    "\n",
    "y1_pred = model1.predict(X1_test)\n",
    "mse1 = mean_squared_error(y1_test, y1_pred)\n",
    "mae1 = mean_absolute_error(y1_test, y1_pred)\n",
    "print(\"Mean Squared Error for K-NN Model1:\", mse1)\n",
    "print(\"Mean Absolute Error for K-NN Model1:\", mae1)\n",
    "\n",
    "# Creating a figure and axis.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the distribution of actual IMDb scores in blue.\n",
    "sns.histplot(y1_test, bins=10, color='blue', alpha=0.5, label='Actual', kde=True)\n",
    "\n",
    "# Plotting the distribution of predicted IMDb scores in red.\n",
    "sns.histplot(y1_pred, bins=10, color='red', alpha=0.5, label='Predicted', kde=True)\n",
    "\n",
    "# Adding labels and a legend.\n",
    "plt.xlabel('IMDb Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted IMDb Scores for KNN Method')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot.\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09793e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('fixed_training_titles.csv')\n",
    "\n",
    "# Dropping rows with missing values in specified columns.\n",
    "df.dropna(subset=['tmdb_score', 'imdb_score'], inplace=True)  \n",
    "\n",
    "def train_model(df, features, target):\n",
    "    '''Trains K-NN models on the provided data.'''\n",
    "\n",
    "    # Defining the features and the target for the model.\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Splitting the data into training and testing sets.\n",
    "    # A common rule is to use about 80% of the data for training and about 20% of the data for testing.\n",
    "    # A common choice is to set random_state to 0 or 42.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the model.\n",
    "    # Fitting the model to the training data.\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Defining the features.\n",
    "selected_features = ['tmdb_score', 'tmdb_popularity']\n",
    "\n",
    "# Training the models.\n",
    "model2, X2_test, y2_test = train_model(df, selected_features, 'imdb_score')\n",
    "\n",
    "score2 = model2.score(X2_test, y2_test)\n",
    "print(f'K-NN Model2 R^2 Score: {score2}')\n",
    "\n",
    "scores2 = cross_val_score(model2, X2_test, y2_test, cv=10)\n",
    "print(f'K-NN Model2 Cross-Validation Score: {scores2.mean()}')\n",
    "\n",
    "y2_pred = model2.predict(X2_test)\n",
    "mse2 = mean_squared_error(y2_test, y2_pred)\n",
    "mae2 = mean_absolute_error(y2_test, y2_pred)\n",
    "print(\"Mean Squared Error for K-NN Model2:\", mse2)\n",
    "print(\"Mean Absolute Error for K-NN Model2:\", mae2)\n",
    "\n",
    "# Creating a figure and axis.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the distribution of actual IMDb scores in blue.\n",
    "sns.histplot(y2_test, bins=10, color='blue', alpha=0.5, label='Actual', kde=True)\n",
    "\n",
    "# Plotting the distribution of predicted IMDb scores in red.\n",
    "sns.histplot(y2_pred, bins=10, color='red', alpha=0.5, label='Predicted', kde=True)\n",
    "\n",
    "# Adding labels and a legend.\n",
    "plt.xlabel('IMDb Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted IMDb Scores for KNN Method')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot.\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('fixed_training_titles.csv')\n",
    "\n",
    "# Dropping rows with missing values in specified columns ('tmdb_score' and 'imdb_score').\n",
    "df.dropna(subset=['tmdb_score', 'imdb_score'], inplace=True)\n",
    "\n",
    "def train_model(df, features, target):\n",
    "    '''Trains Linear Regression models on the provided data.'''    \n",
    "\n",
    "    # Defining the features and the target for the model.\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Splitting the data into training and testing sets.\n",
    "    # A common rule is to use about 80% of the data for training and about 20% of the data for testing.\n",
    "    # A common choice is to set random_state to 0 or 42.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initializing the model.\n",
    "    # Fitting the model to the training data.\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Training the Linear Regression model with 'tmdb_score' as the feature and 'imdb_score' as the target.\n",
    "model, X_test, y_test = train_model(df, ['tmdb_score'], 'imdb_score')\n",
    "\n",
    "# Calculating R-squared, MSE, and MAE.\n",
    "y_pred = model.predict(X_test)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Linear Regression R-squared Score: {r_squared:.2f}')\n",
    "print(\"Mean Squared Error for Linear Regression Model:\", mse)\n",
    "print(\"Mean Absolute Error for Linear Regression Model:\", mae)\n",
    "\n",
    "# Creating a scatterplot to visualize the accuracy with a regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.title('Actual vs. Predicted IMDb Scores')\n",
    "plt.xlabel('Actual IMDb Score')\n",
    "plt.ylabel('Predicted IMDb Scores')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', linewidth=2, label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a figure and axis.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the distribution of actual IMDb scores in blue.\n",
    "sns.histplot(y_test, bins=10, color='blue', alpha=0.5, label='Actual', kde=True)\n",
    "\n",
    "# Plotting the distribution of predicted IMDb scores in red.\n",
    "sns.histplot(y_pred, bins=10, color='red', alpha=0.5, label='Predicted', kde=True)\n",
    "\n",
    "# Adding labels and a legend.\n",
    "plt.xlabel('IMDb Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted IMDb Scores for Linear Regression Model')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac5849",
   "metadata": {},
   "source": [
    "**We will now evaluate the accuracy of using more simple imputation methods such as mode, mean and medians.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('fixed_titles.csv')\n",
    "\n",
    "# Calculating the mode IMDb score excluding NaN values.\n",
    "mode_imdb_score = df['imdb_score'].mode()[0]\n",
    "\n",
    "# Filling NaN values with the mode.\n",
    "df['imdb_score'].fillna(mode_imdb_score, inplace=True)\n",
    "\n",
    "# Calculating the Mean Standard Error (MSE).\n",
    "actual_imdb_scores = df['imdb_score']  # Actual IMDb scores.\n",
    "imputed_imdb_scores = np.full(len(df), mode_imdb_score)  # Imputed IMDb scores (all the same).\n",
    "\n",
    "mse = mean_squared_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "print(f'Mean Squared Error (MSE) using mode for imputation: {mse:.4f}')\n",
    "mae = mean_absolute_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "\n",
    "print(\"Mean Absolute Error using mode for imputation:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d191759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('fixed_titles.csv')\n",
    "\n",
    "# Calculating the mean IMDb score excluding NaN values.\n",
    "mean_imdb_score = df['imdb_score'].mean()\n",
    "\n",
    "# Filling NaN values with the mean.\n",
    "df['imdb_score'].fillna(mean_imdb_score, inplace=True)\n",
    "\n",
    "# Calculating the Mean Standard Error (MSE).\n",
    "actual_imdb_scores = df['imdb_score']  # Actual IMDb scores.\n",
    "imputed_imdb_scores = np.full(len(df), mean_imdb_score)  # Imputed IMDb scores (all the same).\n",
    "\n",
    "mse = mean_squared_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "print(f'Mean Squared Error (MSE) using mean for imputation: {mse:.4f}')\n",
    "mae = mean_absolute_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "\n",
    "print(\"Mean Absolute Error using mean for imputation:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('fixed_titles.csv')\n",
    "\n",
    "# Calculating the median IMDb score excluding NaN values.\n",
    "median_imdb_score = df['imdb_score'].median()\n",
    "\n",
    "# Fill NaN values with the median.\n",
    "df['imdb_score'].fillna(median_imdb_score, inplace=True)\n",
    "\n",
    "# Calculate the Mean Standard Error (MSE)\n",
    "actual_imdb_scores = df['imdb_score']  # Actual IMDb scores.\n",
    "imputed_imdb_scores = np.full(len(df), median_imdb_score)  # Imputed IMDb scores (all the same).\n",
    "\n",
    "mse = mean_squared_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "print(f'Mean Standard Error (MSE) using Median: {mse:.4f}')\n",
    "mae = mean_absolute_error(actual_imdb_scores, imputed_imdb_scores)\n",
    "print(\"Mean Absolute Error using Median for imputation:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0eeea3",
   "metadata": {},
   "source": [
    "**Evidently, the linear regression machine learning model produces the most accurate method of imputation when compared to the use of a decision tree or simple models such as imputation using mean or median. Thus, we will use the linear regression model for imputation of the missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5399e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('fixed_training_titles.csv')\n",
    "\n",
    "# Dropping rows with missing values in specified columns ('tmdb_score' and 'imdb_score').\n",
    "df.dropna(subset=['tmdb_score', 'imdb_score'], inplace=True)\n",
    "\n",
    "# Defining the features and target.\n",
    "features = ['tmdb_score']\n",
    "target = 'imdb_score'\n",
    "\n",
    "# Splitting the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and train a Linear Regression model.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predict_df = pd.read_csv('imdb_score.csv')\n",
    "\n",
    "# Iterating through the DataFrame to fill missing values.\n",
    "for index, row in predict_df.iterrows():\n",
    "    if np.isnan(row['imdb_score']):\n",
    "        predict_df.at[index, 'imdb_score'] = model.predict(np.array([[row['tmdb_score']]]))[0]\n",
    "\n",
    "# Saving the result to a new CSV file.\n",
    "combined_df = pd.concat([predict_df, df], axis=0, ignore_index=True)\n",
    "combined_df.to_csv('final_filled_titles.csv', index=False)\n",
    "\n",
    "# Ensuring all the relevant missing data has actually been filled.\n",
    "df = pd.read_csv('final_filled_titles.csv')\n",
    "df_nan = df.isna().sum()\n",
    "print(df_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06b751",
   "metadata": {},
   "source": [
    "**We have now filled the missing IMDb scores (given that data point had a valid TMDb score). From here, we can use the datafile in conducting natural language processing on text features of the descriptions to investigate the correlations between these text features and the IMDb scores of the shows and films.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a525c",
   "metadata": {},
   "source": [
    "## **Part 3: Description Text Pre-Processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries.\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "titles = pd.read_csv('final_filled_titles.csv') \n",
    "row = titles[titles['id'] == 'tm997728']\n",
    "\n",
    "# Extracting the 'description' column value from the selected row.\n",
    "sample = row['description']\n",
    "sample = sample.reset_index(drop=True)\n",
    "lower_sample = sample[0].lower()\n",
    "\n",
    "# Replacing punctuation characters with spaces in the description text.\n",
    "x = re.sub(\"[-,':\\.]\", \" \", lower_sample)\n",
    "\n",
    "# Splitting the description text into a list of words.\n",
    "word_list = x.split(' ')\n",
    "\n",
    "# Processing the words in the list.\n",
    "for word in word_list:\n",
    "\n",
    "    # Handling single-letter words other than 'a' and 'i'.\n",
    "    if word != 'a' and word != 'i':\n",
    "        if len(word) == 1:\n",
    "            word_list[word_list.index(word) - 1] += word\n",
    "            word_list.remove(word)\n",
    "\n",
    "        # Handling words like 're' and 've' by combining them with the previous word.\n",
    "        elif word == 're' or word == 've':\n",
    "            word_list[word_list.index(word) - 1] += word\n",
    "            word_list.remove(word)\n",
    "\n",
    "    # Removing words with length 0.\n",
    "    if len(word) == 0:\n",
    "        word_list.remove(word)\n",
    "\n",
    "# Creating a string without punctuation.\n",
    "no_punct = ''\n",
    "for word in word_list:\n",
    "    no_punct += word + ' '\n",
    "\n",
    "# Tokenizing the string into words.\n",
    "tokens = nltk.word_tokenize(no_punct)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Removing stopwords from the list of tokens.\n",
    "stopwords_removed = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "# Reconstructing description with stopwords removed.\n",
    "stop_removed_str = ''\n",
    "for word in stopwords_removed:\n",
    "    stop_removed_str = stop_removed_str + word + ' '\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyzing sentiment for the original text and text with stopwords removed.\n",
    "print(f\"Sample description: {lower_sample}\\n{sia.polarity_scores(lower_sample)} \\nWith stopwords removed: {stop_removed_str}\\n{sia.polarity_scores(stop_removed_str)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4355a",
   "metadata": {},
   "source": [
    "**After cleaning and pre-processing the text descriptions we can show that removing stopwords increases the 'visibility' of the sentiment of a given description. Thus we will use these pre-processing techniques in our further analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2db86",
   "metadata": {},
   "source": [
    "## **Part 4: Sentiment Analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0546ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing all descriptions and graphing sentiment vs IMDb score correlation.\n",
    "\n",
    "titles = pd.read_csv('final_filled_titles.csv') \n",
    "titles.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "def preprocess_description_text(row):\n",
    "    '''This function preprocesses the description text by converting it to lower case, removing punctuation,\n",
    "    removing single-letter words (except 'a' and 'i'), removing stopwords, and calculating the sentiment score.'''\n",
    "\n",
    "    description = row['description']\n",
    "\n",
    "    # Converting to lower case and remove punctuation.\n",
    "    # Assumption: All 'description' data are in English. Therefore, only .lower() is needed.\n",
    "    lower_sample = description.lower()\n",
    "    x = re.sub(\"[-,':.]\", \" \", lower_sample)\n",
    "    x = re.sub(\"[-,':\\.]\", \" \", lower_sample)\n",
    "    word_list = x.split(' ')\n",
    "    \n",
    "    # Removing single-letter words (except 'a' and 'i') and certain other words.\n",
    "    for word in word_list:\n",
    "        if word != 'a' and word != 'i':\n",
    "            if len(word) == 1:\n",
    "                word_list[word_list.index(word) - 1] += word\n",
    "                word_list.remove(word)\n",
    "            elif word == 're' or word == 've' or word == 't':\n",
    "                word_list[word_list.index(word) - 1] += word\n",
    "                word_list.remove(word)\n",
    "        if len(word) == 0:\n",
    "            word_list.remove(word)\n",
    "\n",
    "    # Joining words back into a string.\n",
    "    no_punct = ''\n",
    "    for word in word_list:\n",
    "        no_punct += word + ' '\n",
    "\n",
    "    # Tokenizing the string.\n",
    "    tokens = nltk.word_tokenize(no_punct)\n",
    "\n",
    "    # Removing stopwords.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopwords_removed = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "    # Joining words back into a string.\n",
    "    stop_removed_str = ''\n",
    "    for word in stopwords_removed:\n",
    "        stop_removed_str = stop_removed_str + word + ' '\n",
    "    \n",
    "    # Calculating sentiment score.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(stop_removed_str)['compound']  # Getting only the compound score.\n",
    "\n",
    "    return sentiment_score \n",
    "\n",
    "# Applying preprocessing function to each row of the DataFrame.\n",
    "titles['sentiment_score'] = titles.apply(lambda x : preprocess_description_text(x), axis = 1)\n",
    "\n",
    "# Creating scatter plot of sentiment scores vs. IMDb ratings.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(titles['sentiment_score'], titles['imdb_score'], alpha=0.5)\n",
    "plt.title('Sentiment Scores vs. IMDb Score')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('IMDb Score')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculating the Pearson correlation coefficient.\n",
    "pearson_corr = titles['sentiment_score'].corr(titles['imdb_score'], method='pearson')\n",
    "spearman_corr = titles['sentiment_score'].corr(titles['imdb_score'], method='spearman')\n",
    "\n",
    "# Printing the correlation coefficient.\n",
    "print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "print(\"Spearman Correlation Coefficient:\", spearman_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e014e65",
   "metadata": {},
   "source": [
    "The following divides the dataset into the genres and the associated sentiment scores allocated to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty dictionary to store genres.\n",
    "genre_dict = {}\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "# Iterating over the 'genres' column in the 'titles' DataFrame.\n",
    "# Tokenising the genres for ease of use.\n",
    "for i in titles['genres']:\n",
    "\n",
    "    # Removing punctuation from the genres.\n",
    "    pattern = r'[\\\"\\'[\\]]'\n",
    "    wo_punct = re.sub(pattern, '', i)\n",
    "\n",
    "    # Splitting the genres into a list.\n",
    "    lst = wo_punct.split(', ')\n",
    "\n",
    "    # Getting the corresponding sentiment score.\n",
    "    score = titles['sentiment_score'][iterator]\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "    # Adding each genre and its corresponding sentiment score to the dictionary.\n",
    "    for j in lst:\n",
    "        if j not in genre_dict:\n",
    "            genre_dict[j] = []\n",
    "        genre_dict[j].append(score)\n",
    "\n",
    "# Calculating the mean sentiment score for each genre.\n",
    "genre_means = {genre: sum(values) / len(values) for genre, values in genre_dict.items()}\n",
    "\n",
    "# Sorting the genres by their mean sentiment score.\n",
    "genre_means = sorted(genre_means.items(), key=lambda x:x[1])\n",
    "genre_means = dict(genre_means)\n",
    "\n",
    "# Extracting genre names and mean values.\n",
    "genre_names = list(genre_means.keys())\n",
    "genre_means_values = list(genre_means.values())\n",
    "\n",
    "# Creating a bar plot of mean sentiment scores by genre.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(genre_names, genre_means_values)\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Mean sentiment_score')\n",
    "plt.title('Mean sentiment_score by Genre')\n",
    "plt.xticks(rotation=90)  # Rotating genre names for better visibility.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51c09d",
   "metadata": {},
   "source": [
    "**From the analysis above, there is no significant correlation between IMDb scores and the sentiment of a description. Hence, we will analyse the correlation between IMDb score and description using other natural language processing techniques.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c78a6",
   "metadata": {},
   "source": [
    "## **Part 5: Key Word Analysis using TF-IDF Vectorisation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the top words most correlated with IMDb scores.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('final_filled_titles.csv') \n",
    "\n",
    "# Splitting the data into features (descriptions) and target (IMDb ratings).\n",
    "X = df['description']\n",
    "y = df['imdb_score']\n",
    "\n",
    "# Creating a TF-IDF vectorizer to convert descriptions into numerical features.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Fitting a linear regression model to predict IMDb ratings based on TF-IDF features.\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_tfidf, y)\n",
    "\n",
    "# Getting the coefficients of each word in the TF-IDF vectorizer.\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Creating a DataFrame to associate words with their coefficients.\n",
    "coefficients_df = pd.DataFrame({'Word': tfidf_feature_names, 'Coefficient': regressor.coef_})\n",
    "\n",
    "# Sortting the DataFrame by coefficient values to find the words with the highest correlation.\n",
    "top_words = coefficients_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Displaying the top words correlated with high IMDb ratings.\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3c168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709dee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Selecting a subset of top words for visualization (e.g., the top 10 words).\n",
    "selected_words = top_words.head(6)\n",
    "\n",
    "# Initializing a DataFrame to store presence (1 or 0) of selected words.\n",
    "presence_df = X_tfidf[:, [tfidf_vectorizer.vocabulary_[word] for word in selected_words['Word'].values]].toarray()\n",
    "\n",
    "# Creating a DataFrame combining the presence of selected words with IMDb ratings.\n",
    "combined_df = pd.DataFrame(presence_df, columns=selected_words['Word'].values)\n",
    "combined_df['IMDb_Rating'] = y.values\n",
    "\n",
    "# Calculating the number of rows and columns for the grid.\n",
    "num_cols = 3 \n",
    "num_rows = math.ceil(len(selected_words) / num_cols)\n",
    "\n",
    "# Creating a grid of subplots.\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flattening the axes array for easier iteration.\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Creating pairplots for each selected word vs. IMDb rating in separate subplots.\n",
    "for i, word in enumerate(selected_words['Word'].values):\n",
    "    ax = axes[i]\n",
    "    sns.scatterplot(x=word, y='IMDb_Rating', data=combined_df, ax=ax)\n",
    "    \n",
    "    # Adding regression line and calculate correlation coefficient.\n",
    "    reg_line = sns.regplot(x=word, y='IMDb_Rating', data=combined_df, ax=ax, scatter=False, color='red', line_kws={'label': 'Regression Line'})\n",
    "    corr_coef = np.corrcoef(combined_df[word], combined_df['IMDb_Rating'])[0, 1]\n",
    "    \n",
    "    ax.set_title(f'{word} vs. IMDb Rating')\n",
    "    ax.set_xlabel('Frequency of \\'' + word + '\\'')\n",
    "    ax.set_ylabel('IMDb Rating')\n",
    "\n",
    "    # Adding legend for the regression line and correlation value.\n",
    "    ax.legend()\n",
    "    ax.text(0.7, 0.9, f'Correlation: {corr_coef:.2f}', transform=ax.transAxes, fontsize=10, color='blue')\n",
    "\n",
    "# Removing any empty subplots (if the number of selected words is not a multiple of num_cols).\n",
    "for i in range(len(selected_words), num_cols * num_rows):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adding a title for the entire figure.\n",
    "fig.suptitle(\"Pairplots of Selected Words vs. IMDb Rating with Regression Lines and Correlation Values\", y=1.02)\n",
    "\n",
    "# Adjusting the spacing between subplots.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showing the figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "df = pd.read_csv('final_filled_titles.csv')\n",
    "\n",
    "# TF-IDF vectorization of movie descriptions.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['description'])\n",
    "\n",
    "# Calculating the mean TF-IDF value for each movie's description.\n",
    "mean_tfidf = np.mean(X_tfidf.toarray(), axis=1)\n",
    "\n",
    "# Calculating the correlation between the mean TF-IDF values and IMDb scores (Pearson).\n",
    "pearson_corr = np.corrcoef(mean_tfidf, df['imdb_score'])[0, 1]\n",
    "\n",
    "# Calculating the Spearman correlation between the mean TF-IDF values and IMDb scores.\n",
    "spearman_corr, _ = spearmanr(mean_tfidf, df['imdb_score'])\n",
    "\n",
    "# Create a scatter plot with Matplotlib.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(mean_tfidf, df['imdb_score'], alpha=0.5)\n",
    "\n",
    "# Adding a regression line.\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(mean_tfidf, df['imdb_score'])\n",
    "plt.plot(mean_tfidf, slope * mean_tfidf + intercept, color='red', label=f'Regression Line (R={r_value:.2f})')\n",
    "\n",
    "plt.xlabel('Mean TF-IDF Value')\n",
    "plt.ylabel('IMDb Scores')\n",
    "plt.title('Correlation between Mean TF-IDF and IMDb Scores')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Printing the correlations.\n",
    "print(f\"Pearson Correlation between Mean TF-IDF and IMDb Scores: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation between Mean TF-IDF and IMDb Scores: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c0f80",
   "metadata": {},
   "source": [
    "**From the graphs displayed above, it is evident that there are some very loose correlations between the presence of these words and the IMDb score. This is a basis that could be used in our machine learning prediction stage. However, it should be noted that only few of the data points actually contain each of these more highly correlated words. Furthermore, from the Pearson coefficients displayed, these are low values and likely will not be sufficient for an accurate machine learning model unless some other feature selection is introduced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c994d",
   "metadata": {},
   "source": [
    "## **Part 6: Vector Space Analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3870f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv('final_filled_titles.csv')\n",
    "\n",
    "# TF-IDF vectorization of movie descriptions.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['description'])\n",
    "\n",
    "# Applying Kernel PCA to collapse TF-IDF vectors into one dimension.\n",
    "kernel_pca = KernelPCA(n_components=1, kernel='rbf')\n",
    "X_kernel_pca = kernel_pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Calculating the correlation between the Kernel PCA component and IMDb scores.\n",
    "correlation = np.corrcoef(X_kernel_pca.flatten(), df['imdb_score'])[0, 1]\n",
    "\n",
    "# Calculating Pearson correlation between the Kernel PCA component and IMDb scores.\n",
    "pearson_corr, _ = pearsonr(X_kernel_pca.flatten(), df['imdb_score'])\n",
    "print(f\"Pearson Correlation between Kernel PCA Component and IMDb Scores: {pearson_corr:.4f}\")\n",
    "\n",
    "# Calculating Spearman correlation between the Kernel PCA component and IMDb scores.\n",
    "spearman_corr, _ = spearmanr(X_kernel_pca.flatten(), df['imdb_score'])\n",
    "print(f\"Spearman Correlation between Kernel PCA Component and IMDb Scores: {spearman_corr:.4f}\")\n",
    "\n",
    "# Creating a scatter plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_kernel_pca, df['imdb_score'], alpha=0.5)\n",
    "\n",
    "# Adding a regression line.\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X_kernel_pca.flatten(), df['imdb_score'])\n",
    "plt.plot(X_kernel_pca, slope * X_kernel_pca + intercept, color='red', label=f'Regression Line (R={r_value:.2f})')\n",
    "\n",
    "plt.xlabel('Kernel PCA Component')\n",
    "plt.ylabel('IMDb Scores')\n",
    "plt.title('Correlation between Kernel PCA Component and IMDb Scores')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf948ad",
   "metadata": {},
   "source": [
    "## **Part 7: Machine Learning model building and assessing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f9027",
   "metadata": {},
   "source": [
    "**Given the lack of strong Pearson correlation coefficients calculated between description features and IMDb scores throughout the NLP processing stage, we will at this stage implement a number of machine learning strategies such as Decision Trees and Random Forests in an attempt to maximise the accuracy of a machine learning model, using the description to predict IMDb scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc225f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('final_filled_titles.csv')\n",
    "\n",
    "# TF-IDF vectorization of movie descriptions\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['description'])\n",
    "\n",
    "# Apply Kernel PCA to collapse TF-IDF vectors into one dimension\n",
    "kernel_pca = KernelPCA(n_components=1, kernel='rbf')\n",
    "X_kernel_pca = kernel_pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_kernel_pca, df['imdb_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a Random Forest regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE, R-squared, MAE, and MSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "percentage_error = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Percentage Error: {percentage_error:.2f}%')\n",
    "\n",
    "# Creating scatter plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual IMDb Score')\n",
    "plt.ylabel('Predicted IMDb Score')\n",
    "plt.title('Actual vs. Predicted IMDb Scores for TF-IDF with PCA Random Forest Method')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Actual IMDb Scores')\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.5, label='Predicted IMDb Scores')\n",
    "plt.xlabel('Kernel PCA Component')\n",
    "plt.ylabel('IMDb Score')\n",
    "plt.title('Actual vs. Predicted IMDb Scores')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08802799",
   "metadata": {},
   "source": [
    "**As is expressed through the metrics displayed above, this machine learning model is not as accurate as we would have hoped. However, due to the subjective and wildly variable nature of IMDb ratings, given the many other factors that influence the IMDb ratings including the actual quality of the film, the plot and the actors, this model is relatively good at predicting the IMDb score of a movie/tv show through the calculation of the TF-IDF vector of the description of that film, undertaking principal component analysis to do dimension reduction to get the final PCA component value. The mean absolute error of 1.06 means that this model predicts the IMDb scores to an accuracy of +- 1.06 IMDb values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histogram.\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_test, color='blue', alpha=0.5, label='Actual IMDb Scores', kde=True)\n",
    "sns.histplot(y_pred, color='red', alpha=0.5, label='Predicted IMDb Scores', kde=True)\n",
    "plt.xlabel('IMDb Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs. Predicted IMDb Scores for TF-IDF with PCA Method ')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939d1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320a3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
